{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import datetime as datetime\n",
    "import glob\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "import gym\n",
    "import gym_banana\n",
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pad = pd.read_csv('PadData_v2.csv')\n",
    "Pad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "T = 3617\n",
    "m = 10\n",
    "to_draw = np.sort(Pad['timestamp'].unique())\n",
    "ccy = np.sort(Pad['currency pair'].unique())\n",
    "min_history = 500 # min episode length\n",
    "\n",
    "    \n",
    "def generate_episode(n,cur):\n",
    "    _max = to_draw.shape[0]\n",
    "    _end = min(n+T, _max)\n",
    "    timeframe = to_draw[n:_end]\n",
    "    other_bid = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    other_ask = np.zeros((timeframe.shape[0],ccy.shape[0]-1))\n",
    "    i = 0\n",
    "    for elem in ccy:\n",
    "        tmp = Pad[Pad['currency pair'] == elem]\n",
    "        if elem == cur:\n",
    "            target_bid = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            target_ask = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "        else:\n",
    "            other_bid[:,i] = tmp[tmp.timestamp.isin(timeframe)]['bid price'].values\n",
    "            other_ask[:,i] = tmp[tmp.timestamp.isin(timeframe)]['ask price'].values\n",
    "            i += 1\n",
    "    return target_bid, target_ask, other_bid, other_ask\n",
    "\n",
    "def features(price_path,m):\n",
    "    features = np.zeros((price_path.shape[0]-m,m))\n",
    "    for i in range(m):\n",
    "        features[:,i] = (np.log(price_path) - np.log(np.roll(price_path, i+1)))[m:]\n",
    "    return features\n",
    "\n",
    "def get_features(target_bid, target_ask, other_bid, other_ask, m):\n",
    "    feature_span = features(target_bid,m)\n",
    "    feature_span = np.append(feature_span, features(target_ask,m), axis = 1)\n",
    "    for i in range(other_bid.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_bid[:,i],m), axis = 1)\n",
    "    for j in range(other_ask.shape[1]):\n",
    "        feature_span = np.append(feature_span, features(other_ask[:,j],m), axis = 1)\n",
    "    return feature_span\n",
    "\n",
    "def draw_episode(m, cur, min_history):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "    n = np.random.randint(to_draw.shape[0] - min_history)\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    normalized = (feature_span-feature_span.mean())/feature_span.std()\n",
    "    return target_bid, target_ask, normalized\n",
    "\n",
    "def draw_train_episode(m, cur, min_history):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "    to_draw_train = to_draw[:int(to_draw.shape[0]*0.6)]\n",
    "    n = np.random.randint(to_draw_train.shape[0] - min_history)\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    normalized = (feature_span-feature_span.mean())/feature_span.std()\n",
    "    return target_bid, target_ask, normalized\n",
    "\n",
    "def draw_test_episode(m, cur, min_history):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "    to_draw_test = to_draw[int(to_draw.shape[0]*0.8):]\n",
    "    n = np.random.randint(to_draw_test.shape[0] - min_history)\n",
    "    n = 1\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    normalized = (feature_span-feature_span.mean())/feature_span.std()\n",
    "    return target_bid, target_ask, normalized\n",
    "\n",
    "def draw_eval_episode(m, cur, min_history,  offset):\n",
    "    '''\n",
    "    Input:\n",
    "        m, number of lag returns z_1,...z_m\n",
    "        cur, currency pair that we target to trade\n",
    "        min_history, min length of a valid episode\n",
    "    '''\n",
    "    n = int(to_draw.shape[0] * 0.6) + (offset * 3000 % (int(to_draw.shape[0] * 0.8) - int(to_draw.shape[0]*0.6)))\n",
    "    target_bid, target_ask, other_bid, other_ask = generate_episode(n,cur)\n",
    "    feature_span = get_features(target_bid, target_ask, other_bid, other_ask, m)\n",
    "    normalized = (feature_span-feature_span.mean())/feature_span.std()\n",
    "    return target_bid, target_ask, normalized\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_banana\n",
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "env = gym.make('Banana-v0')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(256, 1, bias = True)\n",
    "        torch.nn.init.xavier_uniform(self.affine1.weight)\n",
    "        #print('The weight of the affine layer is', self.affine1.weight)\n",
    "#         self.dropout = nn.Dropout(p=0)\n",
    "#        self.mu = Variable(torch.randn(3, 3), requires_grad=True)\n",
    "        #self.mu = nn.Linear(1,3, bias = True)\n",
    "                #torch.nn.init.xavier_uniform(self.affine1.weight)\n",
    "        #print('The weight of the affine layer is', self.affine1.weight)\n",
    "        # self.tanh = nn.Tanh()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = 0\n",
    "        self.actions = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "#         x = torch.matmul(x, self.A) \n",
    "#         x = self.dropout(x)\n",
    "#         print(x.size())\n",
    "        #mu_sigma = torch.matmul(y, self.mu)\n",
    "#         print(mu_sigma.size())\n",
    "        # action = self.tanh(x + mu_sigma)\n",
    "        # action = self.softmax(action)\n",
    "        action = self.tanh(x)\n",
    "#         action_scores = self.affine2(x)\n",
    "#         return F.softmax(action_scores, dim=1)\n",
    "        return action\n",
    "\n",
    "\n",
    "global policy\n",
    "policy = Policy()\n",
    "NUM_OF_EVAL_DATA = 10\n",
    "PATH = './best_model_AUDUSD.pth'\n",
    "optimizer = optim.SGD(policy.parameters(), lr=1e-1)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def main():\n",
    "    best_accumulative_return = 0\n",
    "    rewards_over_time = []\n",
    "    for  epoch in range(50):\n",
    "        for i_episode in range(50):\n",
    "            ask = np.zeros((1, 1))\n",
    "            bid = np.zeros((1,1 ))\n",
    "            previous_action = 0\n",
    "            while ask.shape[0] <= 3600 and bid.shape[0]<=3600:\n",
    "                target_bid, target_ask, feature_span = draw_train_episode(16, 'AUDUSD', 1000)\n",
    "                bid, ask, features = target_bid[1:]*1e3, target_ask[1:]*1e3, feature_span\n",
    "            for t in range(3600):  # Don't infinite loop while learning\n",
    "                state = feature_span[t]\n",
    "                save_action = policy(torch.from_numpy(state).float())\n",
    "                #price_change = (ask[t+1] - ask[t]) + (bid[t+1] - bid[t])\n",
    "                if t == 3599:\n",
    "                    save_action = 0\n",
    "                action = save_action - previous_action\n",
    "\n",
    "                price = 0\n",
    "                if action > 0:\n",
    "                    price = ask[t] \n",
    "                elif action < 0:\n",
    "                    price = bid[t]\n",
    "                reward = torch.sum(-1 * action * price)  \n",
    "                \n",
    "\n",
    "                policy.rewards += reward\n",
    "            \n",
    "                previous_action = save_action\n",
    "            optimizer.zero_grad()\n",
    "            loss = - policy.rewards / 3600\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            if i_episode %  10 ==  0:\n",
    "                print('Epoch: {} Episode:{} The loss of training is {}'.format(epoch, i_episode, loss.item()))\n",
    "            policy.rewards = 0\n",
    "        # test after running 1000 episodes\n",
    "        policy.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            accumulative_reward_test = 0\n",
    "            for j in range(NUM_OF_EVAL_DATA):\n",
    "                current_reward = 0\n",
    "                ask = np.zeros((1, 1))\n",
    "                bid = np.zeros((1,1 ))\n",
    "                previous_action = 0\n",
    "                while ask.shape[0] <= 3600 and bid.shape[0]<=3600:\n",
    "                    target_bid, target_ask, feature_span = draw_eval_episode(16, 'AUDUSD', 1000, j)\n",
    "                    bid, ask, features = target_bid[1:]*1e3, target_ask[1:]*1e3, feature_span\n",
    "                for t in range(3600):  # Don't infinite loop while learning\n",
    "                    state = feature_span[t]\n",
    "                    save_action = policy(torch.from_numpy(state).float())\n",
    "                    #price_change = (ask[t+1] - ask[t]) + (bid[t+1] - bid[t])\n",
    "                    if t == 3599:\n",
    "                        save_action = 0\n",
    "                    action = save_action - previous_action\n",
    "\n",
    "                    price = 0\n",
    "                    if action > 0:\n",
    "                        price = ask[t] \n",
    "                    elif action < 0:\n",
    "                        price = bid[t]\n",
    "                    reward = torch.sum(-1 * action * price)          \n",
    "                    accumulative_reward_test += reward\n",
    "                    current_reward  += reward\n",
    "                    previous_action = save_action\n",
    "            print (\"Evaluating on {} datapoint and return is {}\".format(NUM_OF_EVAL_DATA, accumulative_reward_test))\n",
    "            rewards_over_time.append(accumulative_reward_test)\n",
    "            # saving the parameters if the reward is larger than previous one\n",
    "            if (accumulative_reward_test * 1.0 / NUM_OF_EVAL_DATA > best_accumulative_return):\n",
    "                torch.save(policy.state_dict(), PATH)\n",
    "                best_accumulative_return = accumulative_reward_test * 1.0 / NUM_OF_EVAL_DATA \n",
    "        print (80*\"=\")\n",
    "        policy.train()\n",
    "       \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
